{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3662,
     "status": "ok",
     "timestamp": 1591459866974,
     "user": {
      "displayName": "Yunpeng Deng",
      "photoUrl": "",
      "userId": "11828948728824100341"
     },
     "user_tz": -480
    },
    "id": "ErvIucUC98Jd",
    "outputId": "6ac08d1f-1f75-43b1-c720-a5eedc82d7c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3654,
     "status": "ok",
     "timestamp": 1591459866976,
     "user": {
      "displayName": "Yunpeng Deng",
      "photoUrl": "",
      "userId": "11828948728824100341"
     },
     "user_tz": -480
    },
    "id": "GSlUNuJi98Jj",
    "outputId": "d9219f03-f06b-4136-a5a0-813ed0b1dcf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Colab Notebooks/ML2020_spring_crack_detection/CrackU-Net-mean-squared-error\n"
     ]
    }
   ],
   "source": [
    "#进入需要训练项目的文件夹\n",
    "%cd /content/drive/My Drive/Colab Notebooks/ML2020_spring_crack_detection/CrackU-Net-mean-squared-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU相关-------------------TPU相关---------------------TPU相关----------\n",
    "# 安装好TPU计算相应的包\n",
    "import os\n",
    "assert os.environ['COLAB_TPU_ADDR'], 'Make sure to select TPU from Edit > Notebook settings > Hardware accelerator'\n",
    "DIST_BUCKET=\"gs://tpu-pytorch/wheels\"\n",
    "TORCH_WHEEL=\"torch-1.15-cp36-cp36m-linux_x86_64.whl\"\n",
    "TORCH_XLA_WHEEL=\"torch_xla-1.15-cp36-cp36m-linux_x86_64.whl\"\n",
    "TORCHVISION_WHEEL=\"torchvision-0.3.0-cp36-cp36m-linux_x86_64.whl\"\n",
    "\n",
    "# Install Colab TPU compat PyTorch/TPU wheels and dependencies\n",
    "!pip uninstall -y torch torchvision\n",
    "!gsutil cp \"$DIST_BUCKET/$TORCH_WHEEL\" .\n",
    "!gsutil cp \"$DIST_BUCKET/$TORCH_XLA_WHEEL\" .\n",
    "!gsutil cp \"$DIST_BUCKET/$TORCHVISION_WHEEL\" .\n",
    "!pip install \"$TORCH_WHEEL\"\n",
    "!pip install \"$TORCH_XLA_WHEEL\"\n",
    "!pip install \"$TORCHVISION_WHEEL\"\n",
    "!sudo apt-get install libomp5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU相关-------------------TPU相关---------------------TPU相关----------\n",
    "# 导入相关的库\n",
    "# import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.data_parallel as dp\n",
    "\n",
    "\n",
    "# 后续对于torchTensor类型的量可以使用 .to(xm.xla_device()) 将torch.FloatTensor转换成 xla tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7742,
     "status": "ok",
     "timestamp": 1591459871070,
     "user": {
      "displayName": "Yunpeng Deng",
      "photoUrl": "",
      "userId": "11828948728824100341"
     },
     "user_tz": -480
    },
    "id": "MIA0UYD6YjeS",
    "outputId": "d0d6c27b-e9e4-42da-ea25-098f08f592c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlutils in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
      "Requirement already satisfied: xlrd>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from xlutils) (1.1.0)\n",
      "Requirement already satisfied: xlwt>=0.7.4 in /usr/local/lib/python3.6/dist-packages (from xlutils) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xlutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBSpRY0i98Ju"
   },
   "outputs": [],
   "source": [
    "from models_crack_unet import SegmentNet, weights_init_normal\n",
    "from dataset_crack_unet import CFDDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import PIL.Image as Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import xlwt\n",
    "import xlrd\n",
    "from xlutils.copy import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU相关-------------------TPU相关---------------------TPU相关----------\n",
    "# 获取TPU的设备数，并且根据设备数准备并行计算\n",
    "devices = (xm.get_xla_supported_devices(max_devices=num_cores) if num_cores != 0 else [])\n",
    "\n",
    "print(\"Devices: {}\".format(devices))\n",
    "\n",
    "# 使用TPU设备来运行模型\n",
    "# model_parallel = dp.DataParallel(SegmentNet, device_ids=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8681,
     "status": "ok",
     "timestamp": 1591459872021,
     "user": {
      "displayName": "Yunpeng Deng",
      "photoUrl": "",
      "userId": "11828948728824100341"
     },
     "user_tz": -480
    },
    "id": "KrIvveOM98Jz",
    "outputId": "ccd4ab91-e2a3-4998-db48-a15e3b45f5ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=4, begin_epoch=2, cuda=True, end_epoch=101, gpu_num=1, img_height=320, img_width=480, lr=0.0005, need_save=True, need_test=True, save_interval=2, test_interval=2, worker_num=0)\n"
     ]
    }
   ],
   "source": [
    "# 在ipynb文件中，parse的创建用函数来创建\n",
    "# 直接用parser=parser = argparse.ArgumentParser() 来创建之后调用会报错\n",
    "\n",
    "def get_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--tpu\", type=bool, default=True, help=\"tpu\")\n",
    "    parser.add_argument(\"--tpu_num\", type=int, default=8, help=\"number of gpu\") #TPU共有8个设备\n",
    "    parser.add_argument(\"--worker_num\", type=int, default=4, help=\"number of input workers\") \n",
    "    parser.add_argument(\"--batch_size\", type=int, default=8, help=\"batch size of input\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0005, help=\"adam: learning rate\")\n",
    "    parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "    parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "\n",
    "    parser.add_argument(\"--begin_epoch\", type=int, default=2, help=\"begin_epoch\")\n",
    "    parser.add_argument(\"--end_epoch\", type=int, default=101, help=\"end_epoch\")\n",
    "\n",
    "    parser.add_argument(\"--need_test\", type=bool, default=True, help=\"need to test\")\n",
    "    parser.add_argument(\"--test_interval\", type=int, default=2, help=\"interval of test\")\n",
    "    parser.add_argument(\"--need_save\", type=bool, default=True, help=\"need to save\")\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=2, help=\"interval of save weights\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--img_width\", type=int, default=480, help=\"size of image width\")\n",
    "    parser.add_argument(\"--img_height\", type=int, default=320, help=\"size of image height\")\n",
    "    \n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "opt = get_arguments()\n",
    "\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mnHkdMxx98J7"
   },
   "outputs": [],
   "source": [
    "dataSetRoot = \"../Data\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_lrEnbFb98KA"
   },
   "outputs": [],
   "source": [
    "# 建立网络\n",
    "segment_net = SegmentNet(init_weights=True)\n",
    "\n",
    "# 选择均方误差损失函数\n",
    "criterion_segment  = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bqkKSXpG98KF"
   },
   "outputs": [],
   "source": [
    "# 选择训练环境和参数\n",
    "if opt.tpu:\n",
    "    #segment_net = segment_net.cuda()\n",
    "    #criterion_segment.cuda()\n",
    "    segment_net = dp.DataParallel(SegmentNet, device_ids=devices)\n",
    "    criterion_segment = dp.DataParallel( criterion_segment, device_ids=devices)\n",
    "    \n",
    "#if opt.gpu_num > 1:\n",
    "#    segment_net = torch.nn.DataParallel(segment_net, device_ids=list(range(opt.gpu_num)))\n",
    "\n",
    "if opt.begin_epoch != 0:\n",
    "    # 加载前期训练的模型\n",
    "    segment_net.load_state_dict(torch.load(\"./saved_models/segment_net_%d.pth\" % (opt.begin_epoch)))\n",
    "else:\n",
    "    # 初始化权重\n",
    "    segment_net.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P7x5y3H098KK"
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_seg = torch.optim.Adam(segment_net.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cys8eeMh98KP"
   },
   "outputs": [],
   "source": [
    "# 对原始数据和真实值进行一定前期处理，方便后续训练\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "transforms_mask = transforms.Compose([\n",
    "    transforms.Resize((opt.img_height, opt.img_width)), \n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "STNFdFYe98KU"
   },
   "outputs": [],
   "source": [
    "# 加载训练集和测试集\n",
    "trainCFDloader = DataLoader(\n",
    "    CFDDataset(dataSetRoot, transforms_=transforms_, transforms_mask= transforms_mask, subFold=\"CFD\", isTrain=True),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.worker_num,\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    CFDDataset(dataSetRoot, transforms_=transforms_, transforms_mask= transforms_mask,  subFold=\"CFD/cfd_TEST\", isTrain=True),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.worker_num\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q_Jm7eRbY2G5"
   },
   "outputs": [],
   "source": [
    "# 定义对输出结果进行阈值化处理的函数，将小于阈值的计算为0，大于阈值的计算为1，在图像中像素为1的点为白色，为0的点为黑色\n",
    "def data_threshold(data, threshold):\n",
    "    # threshold = torch.Tensor([threshold]).cuda()\n",
    "    # data_target = torch.Tensor([i//threshold for i in data]).cuda()\n",
    "    threshold = torch.Tensor([threshold]).to(xm.xla_device())\n",
    "    data_target = torch.Tensor([i//threshold for i in data]).to(xm.xla_device())\n",
    "    return data_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zXOomx_1Y2OK"
   },
   "outputs": [],
   "source": [
    "# 获取训练之后的 accuracy, precision , recall ,F1评价指标\n",
    "# mask是真实值，data是预测值\n",
    "def evaluate_metric(mask, data):\n",
    "    \n",
    "    count_TP, count_FN, count_FP, count_TN = 0, 0, 0, 0\n",
    "    \n",
    "    for i in range(len(mask)):\n",
    "        if mask[i]==1 and data[i]==1:\n",
    "            count_TP += 1  \n",
    "        elif mask[i]==1:\n",
    "            count_FN += 1      \n",
    "        elif data[i]==1:\n",
    "            count_FP += 1      \n",
    "        else:\n",
    "            count_TN += 1\n",
    "\n",
    "    count = count_TP + count_FN + count_FP +count_TN\n",
    "    \n",
    "    # 准确率\n",
    "    accuracy = (count_TP+count_TN)/count \n",
    "    # 精准率\n",
    "    precision = count_TP / (count_TP + count_FP) \n",
    "    # 查全率\n",
    "    recall = count_TP / (count_TP + count_FN)\n",
    "    # F1分\n",
    "    F1 = 2*count_TP/(2*count_TP + count_FP + count_FN)\n",
    "    \n",
    "    return accuracy, precision, recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2OyqC9wnY2TQ"
   },
   "outputs": [],
   "source": [
    "if opt.begin_epoch == 0:\n",
    "  # 将训练过程的需要保存的数据保存到xls文件中\n",
    "  # 创建一个workbook，设置编码\n",
    "  workbook = xlwt.Workbook(encoding = 'utf-8')\n",
    "\n",
    "  #---------------------写入训练过程的epoch, loss, accuracy------------------\n",
    "  # 创建一个worksheet\n",
    "  worksheet = workbook.add_sheet('sheet1')\n",
    "  worksheet.write(0, 0, 'epoch')\n",
    "  worksheet.write(0, 1, 'loss')\n",
    "  worksheet.write(0, 2, 'accuracy')\n",
    "\n",
    "  #---------------------写入测试过程的epoch, loss, accuracy------------------\n",
    "  worksheet = workbook.add_sheet('sheet2')\n",
    "  worksheet.write(0, 0, 'epoch')\n",
    "  worksheet.write(0, 1, 'loss')\n",
    "  worksheet.write(0, 2, 'accuracy')\n",
    "                  \n",
    "  #---------------------写入测试过程的epoch, accuracy, precision, recall, F1------------------                \n",
    "  worksheet = workbook.add_sheet('sheet3')\n",
    "  worksheet.write(0, 0, 'epoch')\n",
    "  worksheet.write(0, 1, 'accuracy')\n",
    "  worksheet.write(0, 2, 'precision')\n",
    "  worksheet.write(0, 3, 'recall')\n",
    "  worksheet.write(0, 4, 'F1')\n",
    "\n",
    "  # 保存\n",
    "  workbook.save('evaluate_data.xls')\n",
    "\n",
    "def write_excel_xls_append(path, value, sheet_num):\n",
    "  index = len(value)  # 获取需要写入数据的行数\n",
    "  workbook = xlrd.open_workbook(path)            # 打开工作簿\n",
    "  sheets = workbook.sheet_names()                # 获取工作簿中的所有表格\n",
    "  worksheet = workbook.sheet_by_name(sheets[sheet_num])  # 获取工作簿中所有表格中的的第一个表格\n",
    "  rows_old = worksheet.nrows                 # 获取表格中已存在的数据的行数\n",
    "  new_workbook = copy(workbook)                # 将xlrd对象拷贝转化为xlwt对象\n",
    "  new_worksheet = new_workbook.get_sheet(sheet_num)      # 获取转化后工作簿中的第sheet_num个表格\n",
    "  \n",
    "  for i in range(0, index):\n",
    "      new_worksheet.write(rows_old, i, value[i])  # 追加写入数据\n",
    "  new_workbook.save(path)  # 保存工作簿\n",
    "  print(\"xls格式表格[追加]写入数据成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型的训练函数\n",
    "def train_my_model(epoch, iter_data, lenNum):\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 开始训练\n",
    "    # 记录每一个epoch的总损失和总精度\n",
    "    train_loss_sum, train_acc_sum, batch_count = 0.0, 0.0, 0.0\n",
    "\n",
    "    for i in range(0, lenNum):\n",
    "\n",
    "        # batchData = iterCFD.__next__()\n",
    "        batchData = iterdata.__next__()\n",
    "\n",
    "        # img = batchData[\"img\"].cuda()\n",
    "        # mask = batchData[\"mask\"].cuda()\n",
    "        img = batchData[\"img\"].to(xm.xla_device())\n",
    "        mask = batchData[\"mask\"].to(xm.xla_device())\n",
    "\n",
    "\n",
    "        optimizer_seg.zero_grad()\n",
    "\n",
    "        rst = segment_net(img)\n",
    "        seg = rst[\"seg\"]\n",
    "\n",
    "        # 计算训练过程的损失loss\n",
    "        loss_seg = criterion_segment(seg, mask)\n",
    "        loss_seg.backward()\n",
    "        optimizer_seg.step()\n",
    "\n",
    "        train_loss_sum += loss_seg.item() \n",
    "\n",
    "        # 计算训练过程的accuracy\n",
    "        net_seg = data_threshold(seg.clone().flatten(), 0.6)    # 预测值\n",
    "        mask_seg = mask.clone().flatten()                      # 真实值\n",
    "\n",
    "        # 对每个像素点的值进行比较，相同的点计入right_seg \n",
    "        right_seg = torch.eq(net_seg, mask_seg).sum().float().item()\n",
    "        total_num = float(mask.clone().flatten().size()[0])\n",
    "\n",
    "        batch_acc = right_seg/total_num\n",
    "        train_acc_sum += batch_acc\n",
    "\n",
    "        batch_count += 1\n",
    "\n",
    "        # 输出每个epoch之中每个batch的信息\n",
    "        print(\"[Epocn:{0}],[batch_count:{1}],[loss:{2:.6f}],[accuracy:{3:.6f}]\".format(epoch, batch_count, loss_seg.item(), batch_acc))\n",
    "\n",
    "    # 输出训练过程每个epoch平均的loss和accuracy\n",
    "    print(\"[Epoch {0}/{1}], [loss:{2:.6f}], accuracy:{3:.6f}]\".format(epoch, opt.end_epoch, train_loss_sum/batch_count, train_acc_sum/batch_count))\n",
    "\n",
    "    # 将上述epoch, loss, accuracy数据写入xls文件\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"开始写入训练过程的epoch, loss, accuracy\")\n",
    "    train_xls_value = [epoch, train_loss_sum/batch_count, train_acc_sum/batch_count]\n",
    "    write_excel_xls_append(\"evaluate_data.xls\", train_xls_value, 0)\n",
    "    print(\"------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型的保存函数\n",
    "def save_my_model(epoch, segment_net, need_save, save_interval):\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 以一定周期保存训练之后的模型\n",
    "    if need_save and epoch % save_interval == 0 and epoch >= save_interval:\n",
    "        save_path_str = \"./saved_models\"\n",
    "        if os.path.exists(save_path_str) == False:\n",
    "            os.makedirs(save_path_str, exist_ok=True)\n",
    "\n",
    "    torch.save(segment_net.state_dict(), \"%s/segment_net_%d.pth\" % (save_path_str, epoch))\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"save weights ! epoch = %d\" %epoch)\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型的测试函数\n",
    "def test_my_model(epoch, segment_net, criterion_segment, testloader, need_test, test_interval):\n",
    "    # -----------------------------------------------------------------------------\n",
    "    # 对模型进行测试，并保存结果\n",
    "    if need_test and epoch % test_interval == 0 and epoch >= test_interval:\n",
    "        test_loss_sum, test_acc_sum, batch_count = 0.0, 0.0, 0.0\n",
    "        result_evaluate_epoch = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for i, testBatch in enumerate(testloader):\n",
    "        # imgTest = testBatch[\"img\"].cuda()\n",
    "        imgTest = testBatch[\"img\"].to(xm.xla_device())\n",
    "        t1 = time.time()\n",
    "        rstTest = segment_net(imgTest)\n",
    "        t2 = time.time()\n",
    "\n",
    "        # 计算测试过程的损失loss\n",
    "        # mask = testBatch[\"mask\"].cuda()\n",
    "        mask = testBatch[\"mask\"].to(xm.xla_device())\n",
    "        loss_test = criterion_segment(rstTest[\"seg\"], mask)\n",
    "\n",
    "        test_loss_sum += loss_test.item()\n",
    "\n",
    "        # 计算测试过程的accuracy\n",
    "        net_seg = data_threshold(rstTest[\"seg\"].clone().flatten(), 0.6)  # 预测值\n",
    "        mask_seg = mask.clone().flatten()                  # 真实值\n",
    "\n",
    "        # 对每个像素点的值进行比较，相同的点计入right_seg \n",
    "        right_seg = torch.eq(net_seg, mask_seg).sum().float().item()\n",
    "        total_num = float(mask.clone().flatten().size()[0])\n",
    "\n",
    "        batch_acc = right_seg/total_num\n",
    "        test_acc_sum += batch_acc\n",
    "\n",
    "        batch_count += 1\n",
    "\n",
    "        # 对一个batch测试结果进行综合评估，并进行累加，方便后续保存\n",
    "        result_evaluate_batch = np.array(list(evaluate_metric(mask_seg, net_seg)))\n",
    "        result_evaluate_epoch += result_evaluate_batch\n",
    "\n",
    "        # 对保存的图片进行阈值化处理\n",
    "        seg_shape = rstTest[\"seg\"].data.shape\n",
    "        segTest_flatten = data_threshold(rstTest[\"seg\"].flatten(), 0.6)\n",
    "        segTest = segTest_flatten.reshape(seg_shape[0], seg_shape[1], seg_shape[2], seg_shape[3])\n",
    "\n",
    "        # 建立文件的保存路径\n",
    "        save_path_str = \"./testResultSeg/epoch_%d\"%(epoch)\n",
    "        if os.path.exists(save_path_str) == False:\n",
    "            os.makedirs(save_path_str, exist_ok=True)\n",
    "\n",
    "        # 输出文件的保存信息\n",
    "        print(\"processing image NO %d, time comsuption %fs\"%(i, t2 - t1))\n",
    "        save_image(imgTest.data, \"%s/img_%d.jpg\"% (save_path_str, i))\n",
    "        save_image(segTest.data, \"%s/img_%d_seg.jpg\"% (save_path_str, i))\n",
    "\n",
    "    # 将上述测试过程的评估参数acc,precision,recall和F1分数进行保存\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"开始写入评估参数\")\n",
    "    result_evaluate_epoch = result_evaluate_epoch/np.array([batch_count])\n",
    "    test_xls_metric = [epoch] + list(result_evaluate_epoch)\n",
    "    write_excel_xls_append(\"evaluate_data.xls\", test_xls_metric, 2)\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "    # 输出测试过程每个epoch平均的loss和accuracy\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"[Epoch {0}/{1}], [loss:{2:.6f}], accuracy:{3:.6f}]\".format(epoch, opt.end_epoch, test_loss_sum/batch_count, test_acc_sum/batch_count))\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "    # 将上述epoch, loss, accuracy数据写入xls文件\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"开始写入测试过程的epoch, loss, accuracy\")\n",
    "    test_xls_value = [epoch, test_loss_sum/batch_count, test_acc_sum/batch_count]\n",
    "    write_excel_xls_append(\"evaluate_data.xls\", test_xls_value, 1)\n",
    "    print(\"------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始模型的训练、测试与保存\n",
    "for epoch in range(opt.begin_epoch, opt.end_epoch):\n",
    "    iterCFD = trainCFDloader.__iter__()\n",
    "\n",
    "    lenNum = len(trainCFDloader)\n",
    "\n",
    "    segment_net.train()\n",
    "    \n",
    "    # 模型训练\n",
    "    train_my_model(epoch=epoch, iter_data = iterCFD, lenNum=lenNum)\n",
    "    \n",
    "    # 模型保存\n",
    "    save_my_model(epoch=epoch, segment_net=segment_net, need_save=opt.need_save, save_interval=opy.save_interval)\n",
    "    \n",
    "    # 模型测试\n",
    "    test_my_model(epoch=epoch, segment_net=segment_net, criterion_segment=criterion_segment, \n",
    "                  testloader=testloader , need_test=opt.need_test, test_interval=opt.need_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "AljlVei798Kd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor epoch in range(opt.begin_epoch, opt.end_epoch):\\n\\n  iterCFD = trainCFDloader.__iter__()\\n\\n  lenNum = len(trainCFDloader)\\n\\n  segment_net.train()\\n\\n  # -----------------------------------------------------------------------------\\n  # 开始训练\\n  # 记录每一个epoch的总损失和总精度\\n  train_loss_sum, train_acc_sum, batch_count = 0.0, 0.0, 0.0\\n\\n  for i in range(0, lenNum):\\n      \\n    batchData = iterCFD.__next__()\\n    \\n    # img = batchData[\"img\"].cuda()\\n    # mask = batchData[\"mask\"].cuda()\\n    img = batchData[\"img\"].to(xm.xla_device())\\n    mask = batchData[\"mask\"].to(xm.xla_device())\\n\\n\\n    optimizer_seg.zero_grad()\\n\\n    rst = segment_net(img)\\n    seg = rst[\"seg\"]\\n\\n    # 计算训练过程的损失loss\\n    loss_seg = criterion_segment(seg, mask)\\n    loss_seg.backward()\\n    optimizer_seg.step()\\n\\n    train_loss_sum += loss_seg.item() \\n    \\n    # 计算训练过程的accuracy\\n    net_seg = data_threshold(seg.clone().flatten(), 0.6)    # 预测值\\n    mask_seg = mask.clone().flatten()              # 真实值\\n    \\n    # 对每个像素点的值进行比较，相同的点计入right_seg \\n    right_seg = torch.eq(net_seg, mask_seg).sum().float().item()\\n    total_num = float(mask.clone().flatten().size()[0])\\n    \\n    batch_acc = right_seg/total_num\\n    train_acc_sum += batch_acc\\n    \\n    batch_count += 1\\n\\n    # 输出每个epoch之中每个batch的信息\\n    print(\"[Epocn:{0}],[batch_count:{1}],[loss:{2:.6f}],[accuracy:{3:.6f}]\".format(epoch, batch_count, loss_seg.item(), batch_acc))\\n    \\n  # 输出训练过程每个epoch平均的loss和accuracy\\n  print(\"[Epoch {0}/{1}], [loss:{2:.6f}], accuracy:{3:.6f}]\".format(epoch, opt.end_epoch, train_loss_sum/batch_count, train_acc_sum/batch_count))\\n    \\n  # 将上述epoch, loss, accuracy数据写入xls文件\\n  print(\"------------------------------------------------------------------------------------------\")\\n  print(\"开始写入训练过程的epoch, loss, accuracy\")\\n  train_xls_value = [epoch, train_loss_sum/batch_count, train_acc_sum/batch_count]\\n  write_excel_xls_append(\"evaluate_data.xls\", train_xls_value, 0)\\n  print(\"------------------------------------------------------------------------------------------\")\\n\\n\\n  # -----------------------------------------------------------------------------\\n  # 以一定周期保存训练之后的模型\\n  if opt.need_save and epoch % opt.save_interval == 0 and epoch >= opt.save_interval:\\n\\n    save_path_str = \"./saved_models\"\\n    if os.path.exists(save_path_str) == False:\\n        os.makedirs(save_path_str, exist_ok=True)\\n\\n    torch.save(segment_net.state_dict(), \"%s/segment_net_%d.pth\" % (save_path_str, epoch))\\n    print(\"------------------------------------------------------------------------------------------\")\\n    print(\"save weights ! epoch = %d\" %epoch)\\n    print(\"------------------------------------------------------------------------------------------\")\\n    pass\\n    \\n\\n  # -----------------------------------------------------------------------------\\n  # 对模型进行测试，并保存结果\\n  if opt.need_test and epoch % opt.test_interval == 0 and epoch >= opt.test_interval:\\n\\n    test_loss_sum, test_acc_sum, batch_count = 0.0, 0.0, 0.0\\n    result_evaluate_epoch = np.array([0.0, 0.0, 0.0, 0.0])\\n\\n    for i, testBatch in enumerate(testloader):\\n      # imgTest = testBatch[\"img\"].cuda()\\n      imgTest = testBatch[\"img\"].to(xm.xla_device())\\n      t1 = time.time()\\n      rstTest = segment_net(imgTest)\\n      t2 = time.time()\\n\\n      # 计算测试过程的损失loss\\n      # mask = testBatch[\"mask\"].cuda()\\n      mask = testBatch[\"mask\"].to(xm.xla_device())\\n      loss_test = criterion_segment(rstTest[\"seg\"], mask)\\n\\n      test_loss_sum += loss_test.item()\\n\\n      # 计算测试过程的accuracy\\n      net_seg = data_threshold(rstTest[\"seg\"].clone().flatten(), 0.6)  # 预测值\\n      mask_seg = mask.clone().flatten()                  # 真实值\\n      \\n      # 对每个像素点的值进行比较，相同的点计入right_seg \\n      right_seg = torch.eq(net_seg, mask_seg).sum().float().item()\\n      total_num = float(mask.clone().flatten().size()[0])\\n      \\n      batch_acc = right_seg/total_num\\n      test_acc_sum += batch_acc\\n      \\n      batch_count += 1\\n\\n      # 对一个batch测试结果进行综合评估，并进行累加，方便后续保存\\n      result_evaluate_batch = np.array(list(evaluate_metric(mask_seg, net_seg)))\\n      result_evaluate_epoch += result_evaluate_batch\\n      \\n      # 对保存的图片进行阈值化处理\\n      seg_shape = rstTest[\"seg\"].data.shape\\n      segTest_flatten = data_threshold(rstTest[\"seg\"].flatten(), 0.6)\\n      segTest = segTest_flatten.reshape(seg_shape[0], seg_shape[1], seg_shape[2], seg_shape[3])\\n\\n      # 建立文件的保存路径\\n      save_path_str = \"./testResultSeg/epoch_%d\"%(epoch)\\n      if os.path.exists(save_path_str) == False:\\n          os.makedirs(save_path_str, exist_ok=True)\\n\\n      # 输出文件的保存信息\\n      print(\"processing image NO %d, time comsuption %fs\"%(i, t2 - t1))\\n      save_image(imgTest.data, \"%s/img_%d.jpg\"% (save_path_str, i))\\n      save_image(segTest.data, \"%s/img_%d_seg.jpg\"% (save_path_str, i))\\n\\n    # 将上述测试过程的评估参数acc,precision,recall和F1分数进行保存\\n    print(\"------------------------------------------------------------------------------------------\")\\n    print(\"开始写入评估参数\")\\n    result_evaluate_epoch = result_evaluate_epoch/np.array([batch_count])\\n    test_xls_metric = [epoch] + list(result_evaluate_epoch)\\n    write_excel_xls_append(\"evaluate_data.xls\", test_xls_metric, 2)\\n    print(\"------------------------------------------------------------------------------------------\")\\n    \\n    \\n    # 输出测试过程每个epoch平均的loss和accuracy\\n    print(\"------------------------------------------------------------------------------------------\")\\n    print(\"[Epoch {0}/{1}], [loss:{2:.6f}], accuracy:{3:.6f}]\".format(epoch, opt.end_epoch, test_loss_sum/batch_count, test_acc_sum/batch_count))\\n    print(\"------------------------------------------------------------------------------------------\")\\n    \\n    # 将上述epoch, loss, accuracy数据写入xls文件\\n    print(\"------------------------------------------------------------------------------------------\")\\n    print(\"开始写入测试过程的epoch, loss, accuracy\")\\n    test_xls_value = [epoch, test_loss_sum/batch_count, test_acc_sum/batch_count]\\n    write_excel_xls_append(\"evaluate_data.xls\", test_xls_value, 1)\\n    print(\"------------------------------------------------------------------------------------------\")\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "for epoch in range(opt.begin_epoch, opt.end_epoch):\n",
    "\n",
    "  iterCFD = trainCFDloader.__iter__()\n",
    "\n",
    "  lenNum = len(trainCFDloader)\n",
    "\n",
    "  segment_net.train()\n",
    "\n",
    "  # -----------------------------------------------------------------------------\n",
    "  # 开始训练\n",
    "  # 记录每一个epoch的总损失和总精度\n",
    "  train_loss_sum, train_acc_sum, batch_count = 0.0, 0.0, 0.0\n",
    "\n",
    "  for i in range(0, lenNum):\n",
    "      \n",
    "    batchData = iterCFD.__next__()\n",
    "    \n",
    "    # img = batchData[\"img\"].cuda()\n",
    "    # mask = batchData[\"mask\"].cuda()\n",
    "    img = batchData[\"img\"].to(xm.xla_device())\n",
    "    mask = batchData[\"mask\"].to(xm.xla_device())\n",
    "\n",
    "\n",
    "    optimizer_seg.zero_grad()\n",
    "\n",
    "    rst = segment_net(img)\n",
    "    seg = rst[\"seg\"]\n",
    "\n",
    "    # 计算训练过程的损失loss\n",
    "    loss_seg = criterion_segment(seg, mask)\n",
    "    loss_seg.backward()\n",
    "    optimizer_seg.step()\n",
    "\n",
    "    train_loss_sum += loss_seg.item() \n",
    "    \n",
    "    # 计算训练过程的accuracy\n",
    "    net_seg = data_threshold(seg.clone().flatten(), 0.6)    # 预测值\n",
    "    mask_seg = mask.clone().flatten()              # 真实值\n",
    "    \n",
    "    # 对每个像素点的值进行比较，相同的点计入right_seg \n",
    "    right_seg = torch.eq(net_seg, mask_seg).sum().float().item()\n",
    "    total_num = float(mask.clone().flatten().size()[0])\n",
    "    \n",
    "    batch_acc = right_seg/total_num\n",
    "    train_acc_sum += batch_acc\n",
    "    \n",
    "    batch_count += 1\n",
    "\n",
    "    # 输出每个epoch之中每个batch的信息\n",
    "    print(\"[Epocn:{0}],[batch_count:{1}],[loss:{2:.6f}],[accuracy:{3:.6f}]\".format(epoch, batch_count, loss_seg.item(), batch_acc))\n",
    "    \n",
    "  # 输出训练过程每个epoch平均的loss和accuracy\n",
    "  print(\"[Epoch {0}/{1}], [loss:{2:.6f}], accuracy:{3:.6f}]\".format(epoch, opt.end_epoch, train_loss_sum/batch_count, train_acc_sum/batch_count))\n",
    "    \n",
    "  # 将上述epoch, loss, accuracy数据写入xls文件\n",
    "  print(\"------------------------------------------------------------------------------------------\")\n",
    "  print(\"开始写入训练过程的epoch, loss, accuracy\")\n",
    "  train_xls_value = [epoch, train_loss_sum/batch_count, train_acc_sum/batch_count]\n",
    "  write_excel_xls_append(\"evaluate_data.xls\", train_xls_value, 0)\n",
    "  print(\"------------------------------------------------------------------------------------------\")\n",
    "\n",
    "\n",
    "  # -----------------------------------------------------------------------------\n",
    "  # 以一定周期保存训练之后的模型\n",
    "  if opt.need_save and epoch % opt.save_interval == 0 and epoch >= opt.save_interval:\n",
    "\n",
    "    save_path_str = \"./saved_models\"\n",
    "    if os.path.exists(save_path_str) == False:\n",
    "        os.makedirs(save_path_str, exist_ok=True)\n",
    "\n",
    "    torch.save(segment_net.state_dict(), \"%s/segment_net_%d.pth\" % (save_path_str, epoch))\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"save weights ! epoch = %d\" %epoch)\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    pass\n",
    "    \n",
    "\n",
    "  # -----------------------------------------------------------------------------\n",
    "  # 对模型进行测试，并保存结果\n",
    "  if opt.need_test and epoch % opt.test_interval == 0 and epoch >= opt.test_interval:\n",
    "\n",
    "    test_loss_sum, test_acc_sum, batch_count = 0.0, 0.0, 0.0\n",
    "    result_evaluate_epoch = np.array([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for i, testBatch in enumerate(testloader):\n",
    "      # imgTest = testBatch[\"img\"].cuda()\n",
    "      imgTest = testBatch[\"img\"].to(xm.xla_device())\n",
    "      t1 = time.time()\n",
    "      rstTest = segment_net(imgTest)\n",
    "      t2 = time.time()\n",
    "\n",
    "      # 计算测试过程的损失loss\n",
    "      # mask = testBatch[\"mask\"].cuda()\n",
    "      mask = testBatch[\"mask\"].to(xm.xla_device())\n",
    "      loss_test = criterion_segment(rstTest[\"seg\"], mask)\n",
    "\n",
    "      test_loss_sum += loss_test.item()\n",
    "\n",
    "      # 计算测试过程的accuracy\n",
    "      net_seg = data_threshold(rstTest[\"seg\"].clone().flatten(), 0.6)  # 预测值\n",
    "      mask_seg = mask.clone().flatten()                  # 真实值\n",
    "      \n",
    "      # 对每个像素点的值进行比较，相同的点计入right_seg \n",
    "      right_seg = torch.eq(net_seg, mask_seg).sum().float().item()\n",
    "      total_num = float(mask.clone().flatten().size()[0])\n",
    "      \n",
    "      batch_acc = right_seg/total_num\n",
    "      test_acc_sum += batch_acc\n",
    "      \n",
    "      batch_count += 1\n",
    "\n",
    "      # 对一个batch测试结果进行综合评估，并进行累加，方便后续保存\n",
    "      result_evaluate_batch = np.array(list(evaluate_metric(mask_seg, net_seg)))\n",
    "      result_evaluate_epoch += result_evaluate_batch\n",
    "      \n",
    "      # 对保存的图片进行阈值化处理\n",
    "      seg_shape = rstTest[\"seg\"].data.shape\n",
    "      segTest_flatten = data_threshold(rstTest[\"seg\"].flatten(), 0.6)\n",
    "      segTest = segTest_flatten.reshape(seg_shape[0], seg_shape[1], seg_shape[2], seg_shape[3])\n",
    "\n",
    "      # 建立文件的保存路径\n",
    "      save_path_str = \"./testResultSeg/epoch_%d\"%(epoch)\n",
    "      if os.path.exists(save_path_str) == False:\n",
    "          os.makedirs(save_path_str, exist_ok=True)\n",
    "\n",
    "      # 输出文件的保存信息\n",
    "      print(\"processing image NO %d, time comsuption %fs\"%(i, t2 - t1))\n",
    "      save_image(imgTest.data, \"%s/img_%d.jpg\"% (save_path_str, i))\n",
    "      save_image(segTest.data, \"%s/img_%d_seg.jpg\"% (save_path_str, i))\n",
    "\n",
    "    # 将上述测试过程的评估参数acc,precision,recall和F1分数进行保存\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"开始写入评估参数\")\n",
    "    result_evaluate_epoch = result_evaluate_epoch/np.array([batch_count])\n",
    "    test_xls_metric = [epoch] + list(result_evaluate_epoch)\n",
    "    write_excel_xls_append(\"evaluate_data.xls\", test_xls_metric, 2)\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    \n",
    "    # 输出测试过程每个epoch平均的loss和accuracy\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"[Epoch {0}/{1}], [loss:{2:.6f}], accuracy:{3:.6f}]\".format(epoch, opt.end_epoch, test_loss_sum/batch_count, test_acc_sum/batch_count))\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    # 将上述epoch, loss, accuracy数据写入xls文件\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"开始写入测试过程的epoch, loss, accuracy\")\n",
    "    test_xls_value = [epoch, test_loss_sum/batch_count, test_acc_sum/batch_count]\n",
    "    write_excel_xls_append(\"evaluate_data.xls\", test_xls_value, 1)\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lw30e-j3hoy9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vC6viVZNNnrW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oR2_2Xlphpla"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Yp3AjZWhptu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_segment_crack_unet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
