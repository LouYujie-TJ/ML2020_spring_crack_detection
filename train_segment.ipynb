{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import SegmentNet, DecisionNet, weights_init_normal\n",
    "from dataset import KolektorDataset\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(b1=0.5, b2=0.999, batch_size=2, begin_epoch=0, cuda=True, end_epoch=101, gpu_num=1, img_height=704, img_width=256, lr=0.0005, need_save=True, need_test=True, save_interval=10, test_interval=10, worker_num=0)\n"
     ]
    }
   ],
   "source": [
    "# 在ipynb文件中，parse的创建用函数来创建\n",
    "# 直接用parser=parser = argparse.ArgumentParser() 来创建之后调用会报错\n",
    "\n",
    "def get_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--cuda\", type=bool, default=True, help=\"number of gpu\")\n",
    "    parser.add_argument(\"--gpu_num\", type=int, default=1, help=\"number of gpu\")\n",
    "    parser.add_argument(\"--worker_num\", type=int, default=0, help=\"number of input workers\") # 只有一个GPU,default=0表示单进程加载\n",
    "    parser.add_argument(\"--batch_size\", type=int, default=2, help=\"batch size of input\")\n",
    "    parser.add_argument(\"--lr\", type=float, default=0.0005, help=\"adam: learning rate\")\n",
    "    parser.add_argument(\"--b1\", type=float, default=0.5, help=\"adam: decay of first order momentum of gradient\")\n",
    "    parser.add_argument(\"--b2\", type=float, default=0.999, help=\"adam: decay of first order momentum of gradient\")\n",
    "\n",
    "    parser.add_argument(\"--begin_epoch\", type=int, default=0, help=\"begin_epoch\")\n",
    "    parser.add_argument(\"--end_epoch\", type=int, default=101, help=\"end_epoch\")\n",
    "\n",
    "    parser.add_argument(\"--need_test\", type=bool, default=True, help=\"need to test\")\n",
    "    parser.add_argument(\"--test_interval\", type=int, default=10, help=\"interval of test\")\n",
    "    parser.add_argument(\"--need_save\", type=bool, default=True, help=\"need to save\")\n",
    "    parser.add_argument(\"--save_interval\", type=int, default=10, help=\"interval of save weights\")\n",
    "\n",
    "\n",
    "    parser.add_argument(\"--img_height\", type=int, default=320, help=\"size of image height\")\n",
    "    parser.add_argument(\"--img_width\", type=int, default=480, help=\"size of image width\")\n",
    "    \n",
    "    return parser.parse_args(args=[])\n",
    "\n",
    "opt = get_arguments()\n",
    "\n",
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSetRoot = \"./Data\" #\"/home/sean/Data/KolektorSDD_sean\"  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build nets\n",
    "segment_net = SegmentNet(init_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_segment  = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用GPU计算损失函数\n"
     ]
    }
   ],
   "source": [
    "# 进行GPU，预处理模型以及初始化权重操作\n",
    "if opt.cuda:\n",
    "    segment_net = segment_net.cuda()\n",
    "    criterion_segment.cuda()\n",
    "    print(\"用GPU计算损失函数\")\n",
    "\n",
    "\n",
    "if opt.gpu_num > 1:\n",
    "    segment_net = torch.nn.DataParallel(segment_net, device_ids=list(range(opt.gpu_num)))\n",
    "\n",
    "if opt.begin_epoch != 0:\n",
    "    # Load pretrained models\n",
    "    segment_net.load_state_dict(torch.load(\"./saved_models/segment_net_%d.pth\" % (opt.begin_epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    segment_net.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_seg = torch.optim.Adam(segment_net.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "# transforms.Compose 串联多个图片变换的操作\n",
    "transforms_ = transforms.Compose([\n",
    "    \n",
    "    # Resize PIL图像，如果size为（h, w）,则将图像resize成这个尺寸\n",
    "    # 使用 Image.BICUBIC插值方法，可以获得较高分辨率的图像\n",
    "    transforms.Resize((opt.img_height, opt.img_width), Image.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "transforms_mask = transforms.Compose([\n",
    "    transforms.Resize((opt.img_height, opt.img_width)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据加载\n",
    "\n",
    "trainCFDloader = DataLoader(\n",
    "    KolektorDataset(dataSetRoot, transforms_=transforms_, transforms_mask= transforms_mask, subFold=\"CFD\", isTrain=True),\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=opt.worker_num,\n",
    ")\n",
    "\n",
    "testloader = DataLoader(\n",
    "    KolektorDataset(dataSetRoot, transforms_=transforms_, transforms_mask= transforms_mask,  subFold=\"CFD/cfd_TEST\", isTrain=False),\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=opt.worker_num,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/101], [loss:0.15797538219273768]\n",
      "[Epoch 1/101], [loss:0.025307815437289802]\n",
      "[Epoch 2/101], [loss:0.023886030095375398]\n",
      "[Epoch 3/101], [loss:0.019246569263156165]\n",
      "[Epoch 4/101], [loss:0.016922109146517785]\n",
      "[Epoch 5/101], [loss:0.015620296397669748]\n",
      "[Epoch 6/101], [loss:0.013027511261911555]\n",
      "[Epoch 7/101], [loss:0.014032604659653523]\n",
      "[Epoch 8/101], [loss:0.012042446940375323]\n",
      "[Epoch 9/101], [loss:0.01137304164215245]\n",
      "[Epoch 10/101], [loss:0.01138983893377537]\n",
      "processing image NO 0, time comsuption 0.007018s\n",
      "processing image NO 1, time comsuption 0.007015s\n",
      "processing image NO 2, time comsuption 0.006982s\n",
      "processing image NO 3, time comsuption 0.007979s\n",
      "processing image NO 4, time comsuption 0.007978s\n",
      "processing image NO 5, time comsuption 0.006982s\n",
      "processing image NO 6, time comsuption 0.006981s\n",
      "processing image NO 7, time comsuption 0.006983s\n",
      "processing image NO 8, time comsuption 0.007016s\n",
      "processing image NO 9, time comsuption 0.006021s\n",
      "processing image NO 10, time comsuption 0.006019s\n",
      "processing image NO 11, time comsuption 0.007015s\n",
      "processing image NO 12, time comsuption 0.010972s\n",
      "processing image NO 13, time comsuption 0.006982s\n",
      "processing image NO 14, time comsuption 0.005984s\n",
      "processing image NO 15, time comsuption 0.006981s\n",
      "processing image NO 16, time comsuption 0.007016s\n",
      "processing image NO 17, time comsuption 0.006983s\n",
      "processing image NO 18, time comsuption 0.005950s\n",
      "processing image NO 19, time comsuption 0.006982s\n",
      "processing image NO 20, time comsuption 0.004987s\n",
      "processing image NO 21, time comsuption 0.005020s\n",
      "processing image NO 22, time comsuption 0.005984s\n",
      "processing image NO 23, time comsuption 0.005982s\n",
      "processing image NO 24, time comsuption 0.005022s\n",
      "processing image NO 25, time comsuption 0.005026s\n",
      "processing image NO 26, time comsuption 0.005955s\n",
      "processing image NO 27, time comsuption 0.005991s\n",
      "processing image NO 28, time comsuption 0.005012s\n",
      "processing image NO 29, time comsuption 0.005953s\n",
      "processing image NO 30, time comsuption 0.005023s\n",
      "save weights ! epoch = 10\n",
      "[Epoch 11/101], [loss:0.00998382151804187]\n",
      "[Epoch 12/101], [loss:0.009778837636324832]\n",
      "[Epoch 13/101], [loss:0.009184220705223694]\n",
      "[Epoch 14/101], [loss:0.008239137977150014]\n",
      "[Epoch 15/101], [loss:0.007558254149361429]\n",
      "[Epoch 16/101], [loss:0.007160941551608796]\n",
      "[Epoch 17/101], [loss:0.007204880279129033]\n",
      "[Epoch 18/101], [loss:0.007369949821044098]\n",
      "[Epoch 19/101], [loss:0.006343987624859437]\n",
      "[Epoch 20/101], [loss:0.006533867071001706]\n",
      "processing image NO 0, time comsuption 0.004986s\n",
      "processing image NO 1, time comsuption 0.004986s\n",
      "processing image NO 2, time comsuption 0.005984s\n",
      "processing image NO 3, time comsuption 0.005983s\n",
      "processing image NO 4, time comsuption 0.005021s\n",
      "processing image NO 5, time comsuption 0.005013s\n",
      "processing image NO 6, time comsuption 0.004987s\n",
      "processing image NO 7, time comsuption 0.005982s\n",
      "processing image NO 8, time comsuption 0.005021s\n",
      "processing image NO 9, time comsuption 0.008012s\n",
      "processing image NO 10, time comsuption 0.005991s\n",
      "processing image NO 11, time comsuption 0.004985s\n",
      "processing image NO 12, time comsuption 0.005984s\n",
      "processing image NO 13, time comsuption 0.004994s\n",
      "processing image NO 14, time comsuption 0.004985s\n",
      "processing image NO 15, time comsuption 0.005990s\n",
      "processing image NO 16, time comsuption 0.005023s\n",
      "processing image NO 17, time comsuption 0.005017s\n",
      "processing image NO 18, time comsuption 0.005957s\n",
      "processing image NO 19, time comsuption 0.004986s\n",
      "processing image NO 20, time comsuption 0.004995s\n",
      "processing image NO 21, time comsuption 0.004987s\n",
      "processing image NO 22, time comsuption 0.020954s\n",
      "processing image NO 23, time comsuption 0.005001s\n",
      "processing image NO 24, time comsuption 0.004986s\n",
      "processing image NO 25, time comsuption 0.005014s\n",
      "processing image NO 26, time comsuption 0.004987s\n",
      "processing image NO 27, time comsuption 0.005991s\n",
      "processing image NO 28, time comsuption 0.004983s\n",
      "processing image NO 29, time comsuption 0.004992s\n",
      "processing image NO 30, time comsuption 0.006980s\n",
      "save weights ! epoch = 20\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-f24e4915221d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0moptimizer_seg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mtrain_loss_sum\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_seg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[1;31m#train_acc_sum += (seg.argmax(dim=1)==mask).sum().item()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mbatch_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "# 模型训练\n",
    "\n",
    "\n",
    "for epoch in range(opt.begin_epoch, opt.end_epoch):\n",
    "    \n",
    "    # Dataset,DataLoder,DataLoderIter，三者是依次封装的关系，前者被封装进入后者\n",
    "    # DataLoder使用__iter__()方法产生成一个DataLoderIter\n",
    "    # 接着后续使用__next__()来得到batch \n",
    "\n",
    "    iterCFD = trainCFDloader.__iter__()\n",
    "    #iterNG = trainNGloader.__iter__()\n",
    "\n",
    "    #lenNum = min( len(trainNGloader), len(trainOKloader))\n",
    "    #lenNum = 2*(lenNum-1)\n",
    "\n",
    "    lenNum = len(trainCFDloader)\n",
    "\n",
    "    segment_net.train()\n",
    "    # train *****************************************************************\n",
    "\n",
    "    # 存储每一个epoch的总损失和总精度\n",
    "    train_loss_sum, train_acc_sum, batch_count = 0.0, 0.0, 0.0\n",
    "\n",
    "    for i in range(0, lenNum):\n",
    "        #if i % 2 == 0:\n",
    "            #batchData = iterOK.__next__()\n",
    "            #idx, batchData = enumerate(trainOKloader)\n",
    "        #else :\n",
    "        #    batchData = iterNG.__next__()\n",
    "            #idx, batchData = enumerate(trainNGloader)\n",
    "        \n",
    "        batchData = iterCFD.__next__()\n",
    "\n",
    "        if opt.cuda:\n",
    "            img = batchData[\"img\"].cuda()\n",
    "            mask = batchData[\"mask\"].cuda()\n",
    "        else:\n",
    "            img = batchData[\"img\"]\n",
    "            mask = batchData[\"mask\"]\n",
    "\n",
    "        optimizer_seg.zero_grad()\n",
    "\n",
    "        rst = segment_net(img)\n",
    "        seg = rst[\"seg\"]\n",
    "\n",
    "        loss_seg = criterion_segment(seg, mask)\n",
    "        loss_seg.backward()\n",
    "        optimizer_seg.step()\n",
    "\n",
    "        train_loss_sum += loss_seg.item() \n",
    "        #train_acc_sum += (seg.argmax(dim=1)==mask).sum().item()\n",
    "        batch_count += 1\n",
    "\n",
    "\n",
    "    #print(\"[Epoch {0}/{1}], [loss:{2}], [accuracy:{3}]\".format(epoch, opt.end_epoch,\n",
    "    #                                                    train_loss_sum/batch_count, train_acc_sum/batch_count))\n",
    "\n",
    "    print(\"[Epoch {0}/{1}], [loss:{2}]\".format(epoch, opt.end_epoch, train_loss_sum/batch_count))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # test ****************************************************************************\n",
    "    if opt.need_test and epoch % opt.test_interval == 0 and epoch >= opt.test_interval:\n",
    "        # segment_net.eval()\n",
    "\n",
    "        for i, testBatch in enumerate(testloader):\n",
    "            imgTest = testBatch[\"img\"].cuda()\n",
    "            t1 = time.time()\n",
    "            rstTest = segment_net(imgTest)\n",
    "            t2 = time.time()\n",
    "            segTest = rstTest[\"seg\"]\n",
    "\n",
    "            save_path_str = \"./testResultSeg/epoch_%d\"%(epoch)\n",
    "            if os.path.exists(save_path_str) == False:\n",
    "                os.makedirs(save_path_str, exist_ok=True)\n",
    "                #os.mkdir(save_path_str)\n",
    "\n",
    "            print(\"processing image NO %d, time comsuption %fs\"%(i, t2 - t1))\n",
    "            save_image(imgTest.data, \"%s/img_%d.jpg\"% (save_path_str, i))\n",
    "            save_image(segTest.data, \"%s/img_%d_seg.jpg\"% (save_path_str, i))\n",
    "        \n",
    "        segment_net.train()\n",
    "\n",
    "    # save parameters *****************************************************************\n",
    "    if opt.need_save and epoch % opt.save_interval == 0 and epoch >= opt.save_interval:\n",
    "        #segment_net.eval()\n",
    "\n",
    "        save_path_str = \"./saved_models\"\n",
    "        if os.path.exists(save_path_str) == False:\n",
    "            os.makedirs(save_path_str, exist_ok=True)\n",
    "\n",
    "        torch.save(segment_net.state_dict(), \"%s/segment_net_%d.pth\" % (save_path_str, epoch))\n",
    "        print(\"save weights ! epoch = %d\" %epoch)\n",
    "        #segment_net.train()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
